suppressPackageStartupMessages({
  library(stringr)
  library(qdap)
  library(tidytext)
  library(tm)
  library(caret)
  library(e1071) 
  library(gbm)
  library(randomForest)
  library(ggplot2)
})

gminwordlen = 4
gprtdecmaxtokens = 2
gprtmaxdigits = 3
gpartitionprob = 0.8
ghighcorrthreshold = 0.9
gruncorr = 1

dirname = "C:/PraveenTayal/GL_Analytics_Pandit/"
setwd(dirname)

df=read.csv("annonymized_data_gl.csv", 
            header=T, na.strings=c("","NA"), stringsAsFactors = FALSE)

dim(df)
str(df)
#drop X (un nammed column)
df$X = NULL
summary(df)

#check NA in Product description feature
nlevels(as.factor(df$Category))
print (sum(is.na(df$Product.Description)))

#--- Utility functions
dummydataframe <- function(){
  return (data.frame(matrix(ncol=0,nrow=0)))
}
CountWords<-function(x){
  str_count(x, '\\w+')
}

UniqueWords<-function(x){ #remove short len words (<=3)
  unique(unlist(strsplit(x, " ")))
}

RemoveShortWords<-function(x){ #remove short len words (<=3)
  y = unlist(strsplit(x, " ")) #lower the case
  paste(y[nchar(y)>=gminwordlen], collapse=" ")
}

RemoveStopWords<-function(x){ 
  x = unlist(rm_stopwords(x, tm::stopwords("english")))
  paste(x, collapse=" ")
}

model.predict.accu<-function(model, df){
  prediction <- predict(model, df)
  xtab <- table(df$Y, prediction)
  A=as.matrix(xtab)
  acc = sum(A[row(A)==col(A)])/nrow(df)
  return (acc)
}

GBMModel.predict.accu<-function(model, df, ntrees){
  
  predict.class <- predict(model,  newdata=df, n.trees = ntrees,
                           type = "response")
  pred_class <- apply(predict.class, 1, which.max)
  xtab <- table(df$Y, pred_class)
  A=as.matrix(xtab)
  acc = sum(A[row(A)==col(A)])/nrow(df)
  return (acc)
}

#fill NA with mean
FillNAWithMean<-function(data){
  for(i in 1:ncol(data)) {
    data[ , i][is.na(data[ , i])] <- mean(data[ , i], na.rm = TRUE)
  }
  return (data)  
}
# ---- Function library ends

prt_dec_col = df$Product.Description
prt_dec_col = str_replace_all(prt_dec_col, "[^a-zA-Z#]", " ") #keep all alpha numeric
prt_dec_col = str_replace_all(prt_dec_col, "[[:punct:]]", " ") #remove all punctuations
prt_dec_col = str_squish(prt_dec_col) # remove extra spaces
prt_dec_col = sapply(prt_dec_col, tolower, USE.NAMES = FALSE)
prt_dec_col = sapply(prt_dec_col, UniqueWords, USE.NAMES = FALSE)
prt_dec_col = sapply(prt_dec_col, RemoveShortWords, USE.NAMES = FALSE)
prt_dec_col = sapply(prt_dec_col, RemoveStopWords, USE.NAMES = FALSE)
prt_dec_col[prt_dec_col==""] = "short_words_misc"
str(prt_dec_col)


#lets count the number of words and look at some stats
prt_dec_col.nwords = sapply(prt_dec_col, CountWords, USE.NAMES = FALSE)

str(prt_dec_col.nwords)
boxplot(prt_dec_col.nwords)

#now get all the words and their counts or frequencies
prt_dec_col.all.words = prt_dec_col %>% 
  sapply(.,paste0,collapse=" ", USE.NAMES = FALSE) %>% 
  strsplit(., " ") %>% 
  unlist() 

prt_dec_col.word.freq = table(prt_dec_col.all.words)
#create a dataframe
all.words.freq.df = cbind(names(prt_dec_col.word.freq),
                          as.integer(prt_dec_col.word.freq)) %>% 
  data.frame() %>%
  dplyr::rename(
    word.name = X1,
    word.freq = X2
  )

all.words.freq.df$word.name = as.character(all.words.freq.df$word.name)
all.words.freq.df$word.freq = as.integer(as.character(
  all.words.freq.df$word.freq))

str(all.words.freq.df)

all.words.freq.df %>% 
  dplyr::arrange(desc(word.freq)) %>% 
  dplyr::slice(1:20) %>%
  ggplot(., aes(x= reorder(word.name, -word.freq), y=word.freq))+
  geom_bar(stat='identity',color='skyblue',fill='steelblue') + 
  theme(axis.text.x=element_text(angle=45, hjust=1))

#calculate tfidf
tdm = TermDocumentMatrix(Corpus(VectorSource(prt_dec_col)),
                         control = list(weighting = weightTfIdf))

freq=rowSums(as.matrix(tdm)) %>%
  sort(., decreasing = T)
head(freq,10)
plot(sort(freq, decreasing = T),
     col="blue",main="Word TF-IDF frequencies", 
     xlab="TF-IDF-based rank", ylab = "TF-IDF")
feature.names=rownames(as.data.frame(freq))
#now we have tfidf score for every word in each row
#lets find the two words that have max scores
tdm.df = as.data.frame(as.matrix(tdm)) 
rm(all.words.freq.df, tdm)
gc()


tdm.df.feat.score.sum <- apply(tdm.df, 1, sum)
tdm.df.feat.score.avg <- apply(tdm.df, 1, mean) #for experimentation
tdm.df.feat.score.max <- apply(tdm.df, 1, max) #for experimentation

##Students can experiment with the following experiments in the code
##sinippet below
#tdmscore = tdm.df.feat.score.sum[word] OR
#tdmscore = tdm.df.feat.score.avg[word] OR
#tdmscore = tdm.df.feat.score.max[word] OR
#tdmscore = tdm.df[word, i]

prt_dec_col.modif = sapply(1:length(prt_dec_col), function(i){
  rowscores = c()
  row.word = c()
  for (word in strsplit(prt_dec_col[i], " ")){
    tdmscore = tdm.df.feat.score.sum[word]
    rowscores = c(rowscores, tdmscore)
    row.word = c(row.word, word)
  }
  word.high.tfidf = head(row.word[order(rowscores, 
                                        decreasing=TRUE)],gprtdecmaxtokens)
  return(paste(word.high.tfidf, collapse = "_"))
}, USE.NAMES = FALSE)

str(prt_dec_col.modif)
head(prt_dec_col.modif,10)
length(unique(prt_dec_col.modif))

##dataframe
df$Product.Description = prt_dec_col.modif#update the dataframe with simplified value
str(df)
rm(tdm.df, prt_dec_col.modif)
gc()

##now treat the Part nos
prt_names_col = df$Product.No
prt_names_col.modif = sapply(prt_names_col, function(x){
  val = gsub(pattern = "\\-",replacement = "",x = x)
  substr(val, 1, gprtmaxdigits) #take only firt 3 digits and siplify
}, USE.NAMES = FALSE)

str(prt_names_col.modif)
length(unique(prt_names_col.modif))
df$Product.No = prt_names_col.modif #update the dataframe with simplified value

##setup for multiclassification
#Clean up the Plant column - converted to Long range dataframe
df = df %>% 
  mutate(Plant_new = strsplit(as.character(Plant), "|", fixed = TRUE)) %>% 
  tidyr::unnest(Plant_new)

df$Plant = NULL
df$Plant_new = as.factor(df$Plant_new)
df = df %>% 
  rename(
    Plant = Plant_new
  )

#outcome variable
Y = as.numeric(as.factor(df$Category))
#all dependent variables
X = dplyr::select(df, -Category)
for(i in 1:ncol(X)){
  X[,i] <- as.factor(X[,i])
}
df.modif = cbind(X, Y)
df.modif$Y = df.modif$Y -1
#split in train and test
set.seed(2106)
train.index <- createDataPartition(df.modif$Y, p = gpartitionprob, list = FALSE)
train <- df.modif[ train.index,]
test  <- df.modif[-train.index,]
dim(train)
dim(test)

rm(prt_names_col, prt_names_col.modif, df)
gc()

#apply catboost model
library(catboost)

train_pool <- catboost.load_pool(data = dplyr::select(train,-Y), 
                                 label = train$Y)
test_pool <- catboost.load_pool(data = dplyr::select(test,-Y), 
                                label = test$Y)

model <- catboost.train(train_pool,  NULL,
                        params = list(loss_function = 'MultiClass',
                                      iterations = 300, metric_period=10,
                                      random_seed = 2106, learning_rate = 0.05,
                                      l2_leaf_reg = 3.5, 
                                      eval_metric='Accuracy',
                                      leaf_estimation_method='Newton',
                                      random_strength=0.1,
                                      depth=7))

#quantization: simple_ctr=c('BinarizedTargetMeanValue')
#model2 is using the mean hot encoding
model2 <- catboost.train(train_pool,  NULL,
                        params = list(loss_function = 'MultiClass',
                                      iterations = 300, metric_period=10,
                                      random_seed = 2106, 
                                      eval_metric='Accuracy',
                                      simple_ctr=c('BinarizedTargetMeanValue')))

train_pred = catboost.predict(model2, train_pool, prediction_type = "Class")
cm_train = confusionMatrix(as.factor(train$Y), as.factor(train_pred))
mean(cm_train$byClass[, "Balanced Accuracy"])

test_pred = catboost.predict(model2, test_pool, prediction_type = "Class")
cm_test = confusionMatrix(as.factor(test$Y), as.factor(test_pred))
mean(cm_test$byClass[, "Balanced Accuracy"])
